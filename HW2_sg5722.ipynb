{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"hw2_data/snli_train.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"hw2_data/snli_val.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "ft_home = 'C:/Users/gong/Documents/learning/NLP/wiki-news-300d-1M.vec/'\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec', encoding = \"utf-8\") as f:\n",
    "    loaded_embeddings_ft = np.zeros((999996, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = []\n",
    "    for i, line in enumerate(f):\n",
    "        if(i == 0):\n",
    "            continue\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[(i+1), :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+1\n",
    "        idx2words_ft[i+1] = s[0]\n",
    "        ordered_words_ft.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = train['sentence1'].values\n",
    "s2 = train['sentence2'].values\n",
    "label = train['label'].values\n",
    "s1_val = val['sentence1'].values\n",
    "s2_val = val['sentence2'].values\n",
    "label_val = val['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = [i.split() for i in s1]\n",
    "s2 = [i.split() for i in s2]\n",
    "s1_val = [i.split() for i in s1_val]\n",
    "s2_val = [i.split() for i in s2_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in s1:\n",
    "    all_words += i\n",
    "for i in s2:\n",
    "    all_words += i\n",
    "max_vocab_size = 20000\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    words_counter = Counter(all_words)\n",
    "    vocab, count = zip(*words_counter.most_common(max_vocab_size))\n",
    "    PAD_IDX = 0 \n",
    "    UNK_IDX = 1\n",
    "    vocab = [i for i in list(vocab) if i in ordered_words_ft]\n",
    "    vocab_size = len(vocab)\n",
    "    loaded_embeddings = np.zeros((vocab_size+2, 300))\n",
    "    for i in range(vocab_size):\n",
    "        loaded_embeddings[(i+2), :] = loaded_embeddings_ft[words_ft[vocab[i]],]\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return  vocab_size, loaded_embeddings, token2id, id2token\n",
    "num_embeddings, loaded_embeddings, token2id, id2token = build_vocab(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18832"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_data = {}\n",
    "voc_data[\"num_embeddings\"] = num_embeddings\n",
    "voc_data[\"loaded_embeddings\"] = loaded_embeddings\n",
    "voc_data[\"token2id\"] =  token2id\n",
    "voc_data[\"id2token\"] = id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl.dump(voc_data, open(\"voc_data_2.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "voc_data = pkl.load(open(\"voc_data_2.p\",\"rb\"))\n",
    "num_embeddings, loaded_embeddings, token2id, id2token = voc_data[\"num_embeddings\"], voc_data[\"loaded_embeddings\"], voc_data[\"token2id\"], voc_data[\"id2token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18834, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "def word_emb(x):\n",
    "    return [token2id[i] if i in token2id else UNK_IDX for i in x]\n",
    "#s1_val = [i.split() for i in s1_val]\n",
    "#s2_val = [i.split() for i in s2_val]\n",
    "s1 = list(map(word_emb, s1))\n",
    "s2 = list(map(word_emb, s2))\n",
    "s1_val = list(map(word_emb, s1_val))\n",
    "s2_val = list(map(word_emb, s2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = {\"s1\": s1,\n",
    "           \"s2\": s2}\n",
    "x_val = {\"s1\": s1_val,\n",
    "           \"s2\": s2_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_label(x):\n",
    "    if(x == \"neutral\"):\n",
    "        return 0\n",
    "    elif(x==\"entailment\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "label = list(map(one_hot_label, label))\n",
    "label_val = list(map(one_hot_label, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        assert len(x[\"s1\"]) == len(y)\n",
    "        self.length = len(y)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "        \"s1\": self.x[\"s1\"][index],\n",
    "        \"s2\": self.x[\"s2\"][index],\n",
    "        \"s1_len\": len(self.x[\"s1\"][index]),\n",
    "        \"s2_len\": len(self.x[\"s2\"][index])\n",
    "        }, self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 82\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    s1_list = []\n",
    "    s2_list = []\n",
    "    s1_len_list = []\n",
    "    s2_len_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[1])\n",
    "        s1_len_list.append(min(MAX_WORD_LENGTH, datum[0][\"s1_len\"]))\n",
    "        s2_len_list.append(min(MAX_WORD_LENGTH, datum[0][\"s2_len\"]))\n",
    "        s1 = np.pad(np.array(datum[0][\"s1\"][:MAX_WORD_LENGTH]),\n",
    "                                pad_width=((0,max(0, MAX_WORD_LENGTH-datum[0][\"s1_len\"]))),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        s1_list.append(s1)\n",
    "        s2 = np.pad(np.array(datum[0][\"s2\"][:MAX_WORD_LENGTH]),\n",
    "                                pad_width=((0,max(0, MAX_WORD_LENGTH-datum[0][\"s2_len\"]))),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        s2_list.append(s2)\n",
    "    ind_dec_order1 = np.argsort(s1_len_list)[::-1]\n",
    "    s1_list = np.array(s1_list)[ind_dec_order1]\n",
    "    s1_len_list = np.array(s1_len_list)[ind_dec_order1]\n",
    "    ind_dec_order2 = np.argsort(s2_len_list)[::-1]\n",
    "    s2_list = np.array(s2_list)[ind_dec_order2]\n",
    "    s2_len_list = np.array(s2_len_list)[ind_dec_order2]       \n",
    "    x = {\n",
    "        \"s1\": torch.from_numpy(np.array(s1_list)).long().to(device),\n",
    "        \"s2\": torch.from_numpy(np.array(s2_list)).long().to(device),\n",
    "        \"s1_len\": torch.LongTensor(s1_len_list).to(device),\n",
    "        \"s2_len\": torch.LongTensor(s2_len_list).to(device),\n",
    "        \"s1_order\": torch.from_numpy(ind_dec_order1.copy()).to(device),\n",
    "        \"s2_order\": torch.from_numpy(ind_dec_order2.copy()).to(device)\n",
    "    }\n",
    "    y = torch.LongTensor(np.array(label_list)[ind_dec_order1])\n",
    "    return x, y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(x_train, label)\n",
    "val_dataset = MyDataset(x_val, label_val)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                collate_fn=vocab_collate_func,\n",
    "                                shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                collate_fn=vocab_collate_func,\n",
    "                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp - (num_embeddings - 2) * emb_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "### baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, middle_size, num_embeddings):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        #\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embed = nn.Embedding(num_embeddings, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        #self.rnn2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(4*hidden_size, middle_size)\n",
    "        self.linear2 = nn.Linear(middle_size, 3)\n",
    "        \n",
    "        self.init_weights()\n",
    "  \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size).float()\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def init_weights(self):\n",
    "        loaded_embeddings[:2, :] = np.random.randn(2,300)\n",
    "        self.embed.weight = nn.Parameter(torch.from_numpy(loaded_embeddings).float())\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reset hidden state\n",
    "        out1 = self.embed(x['s1'])\n",
    "        out2 = self.embed(x['s2'])\n",
    "        batch_size, seq_len = x['s1'].size()\n",
    "        self.hidden1 = self.init_hidden(batch_size).to(device)\n",
    "        self.hidden2 = self.init_hidden(batch_size).to(device)\n",
    "        out1 = torch.nn.utils.rnn.pack_padded_sequence(out1, x['s1_len'], batch_first=True)\n",
    "        out2 = torch.nn.utils.rnn.pack_padded_sequence(out2, x['s2_len'], batch_first=True)\n",
    "        _, self.hidden1 = self.rnn(out1, self.hidden1)\n",
    "        _, self.hidden2 = self.rnn(out2, self.hidden2)\n",
    "        final_hidden1 = self.hidden1.view(1, 2, batch_size, self.hidden_size).transpose(1,2).contiguous().view(batch_size, -1)\n",
    "        final_hidden2 = self.hidden2.view(1, 2, batch_size, self.hidden_size).transpose(1,2).contiguous().view(batch_size, -1)\n",
    "        tmp = torch.zeros(final_hidden2.size()).to(device)\n",
    "        for i in range(batch_size):\n",
    "            tmp[x['s2_order'][i],:] = final_hidden2[i]\n",
    "        final_hidden2 = tmp[list(x['s1_order']),:]\n",
    "        out = torch.cat([final_hidden1,final_hidden2], dim=1)\n",
    "        out = self.linear1(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(data)\n",
    "        loss = criterion(y_hat, labels.long())\n",
    "        loss.backward()\n",
    "        model.embed.weight.grad[2:,:] = torch.zeros(model.embed.weight.grad[2:,:].size()).to(device)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * len(data) / len(dataloader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "def do_eval(model, dataloader):\n",
    "    model.eval()\n",
    "    y_ls = []\n",
    "    y_hat_ls = []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            y_hat = model(data)\n",
    "            loss = criterion(y_hat, labels.long())\n",
    "            y_hat_ls.append(y_hat)\n",
    "            y_ls.append(labels)\n",
    "            val_loss += loss.item() * len(data) / len(dataloader.dataset)\n",
    "    optimizer.zero_grad()\n",
    "    return val_loss, torch.cat(y_hat_ls, dim=0), torch.cat(y_ls, dim=0)\n",
    "\n",
    "def acc(model, dataloader):\n",
    "    val_loss, pred,true = do_eval(\n",
    "    model = model,\n",
    "    dataloader = dataloader,\n",
    "    )\n",
    "    return val_loss, (torch.exp(pred).max(1)[1] == true).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden size = 200\n",
      "Number of parameters is 1325603\n",
      "Epoch: [1/10], Train Loss: 0.08003874897480018, Val Loss: 0.08503139090538026, Val Acc: 0.5940000414848328\n",
      "Epoch: [2/10], Train Loss: 0.07269508834004401, Val Loss: 0.07836294972896575, Val Acc: 0.6330000162124634\n",
      "Epoch: [3/10], Train Loss: 0.0667122457051277, Val Loss: 0.07433435440063477, Val Acc: 0.659000039100647\n",
      "Epoch: [4/10], Train Loss: 0.061480742329359066, Val Loss: 0.07074023938179018, Val Acc: 0.6880000233650208\n",
      "Epoch: [5/10], Train Loss: 0.05649339353263373, Val Loss: 0.0701916482448578, Val Acc: 0.6910000443458557\n",
      "Epoch: [6/10], Train Loss: 0.052581016169786464, Val Loss: 0.06877575552463532, Val Acc: 0.7010000348091125\n",
      "Epoch: [7/10], Train Loss: 0.04771245790421956, Val Loss: 0.07116731357574463, Val Acc: 0.6890000104904175\n",
      "Epoch: [8/10], Train Loss: 0.041403533698618405, Val Loss: 0.07115082263946533, Val Acc: 0.6960000395774841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3265, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-31-ba78b2f0f587>\", line 36, in <module>\n",
      "    train_loss, train_acc = acc(model,train_loader)\n",
      "  File \"<ipython-input-30-4fbe3bd327bd>\", line 32, in acc\n",
      "    dataloader = dataloader,\n",
      "  File \"<ipython-input-30-4fbe3bd327bd>\", line 20, in do_eval\n",
      "    for data, labels in dataloader:\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 314, in __next__\n",
      "    batch = self.collate_fn([self.dataset[i] for i in indices])\n",
      "  File \"<ipython-input-15-8ed3d69c115f>\", line 23, in vocab_collate_func\n",
      "    mode=\"constant\", constant_values=PAD_IDX)\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/numpy/lib/arraypad.py\", line 1276, in pad\n",
      "    newmat = _append_const(newmat, pad_after, after_val, axis)\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/numpy/lib/arraypad.py\", line 161, in _append_const\n",
      "    return _do_append(arr, np.full(padshape, val, dtype=arr.dtype), axis)\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/numpy/lib/arraypad.py\", line 103, in _do_append\n",
      "    (arr, pad_chunk.astype(arr.dtype, copy=False)), axis=axis)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2016, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/stow/python-3.6/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/stow/python-3.6/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/stow/python-3.6/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/sg5722/NLP/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "num_layers = 1\n",
    "middle_size = 200\n",
    "num_epochs = 10\n",
    "hidden_size = 100\n",
    "\n",
    "\n",
    "loss_train_t = []\n",
    "loss_val_t = []\n",
    "acc_val_t = []\n",
    "acc_train_t = []\n",
    "\n",
    "for hidden_size in [100, 300, 500, 700]:\n",
    "    print(\"hidden size = {}\".format(hidden_size))\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    acc_train = []\n",
    "\n",
    "    model = RNN(emb_size, hidden_size, num_layers, middle_size, num_embeddings).to(device)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "    #scheduler.step()\n",
    "        loss = do_train(\n",
    "            model=model, \n",
    "            criterion=criterion,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "        val_loss, val_acc = acc(model, val_loader)\n",
    "        train_loss, train_acc = acc(model,train_loader)\n",
    "        loss_val.append(val_loss)\n",
    "        acc_val.append(val_acc)\n",
    "        loss_train.append(train_loss)\n",
    "        acc_train.append(train_acc)\n",
    "        print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "    loss_train_t.append(loss_train)\n",
    "    loss_val_t.append(loss_val)\n",
    "    acc_val_t.append(acc_val)\n",
    "    acc_train_t.append(acc_train)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(loss_train_t[0])+1), loss_train_t[0], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[0])+1), loss_val_t[0], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[0])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 100\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(loss_train_t[1])+1), loss_train_t[1], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[1])+1), loss_val_t[1], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[1])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 300\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(loss_train_t[2])+1), loss_train_t[2], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[2])+1), loss_val_t[2], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[2])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 500\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(range(1, len(loss_train_t[3])+1), loss_train_t[3], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[3])+1), loss_val_t[3], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[3])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 700\")\n",
    "plt.savefig(\"hidden_rnn_loss.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(acc_train_t[0])+1), acc_train_t[0], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[0])+1), acc_val_t[0], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[0])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 100\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(acc_train_t[1])+1), acc_train_t[1], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[1])+1), acc_val_t[1], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[1])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 300\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(acc_train_t[2])+1), acc_train_t[2], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[2])+1), acc_val_t[2], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[2])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 500\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(range(1, len(acc_train_t[3])+1), acc_train_t[3], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[3])+1), acc_val_t[3], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[3])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 700\")\n",
    "plt.savefig(\"hidden_rnn_acc.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ways of interacting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_max(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, middle_size, num_embeddings):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        #\n",
    "        super(RNN_max, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embed = nn.Embedding(num_embeddings, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        #self.rnn2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(2*hidden_size, middle_size)\n",
    "        self.linear2 = nn.Linear(middle_size, 3)\n",
    "        \n",
    "        self.init_weights()\n",
    "  \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size).float()\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def init_weights(self):\n",
    "        loaded_embeddings[:2, :] = np.random.randn(2,300)\n",
    "        self.embed.weight = nn.Parameter(torch.from_numpy(loaded_embeddings).float())\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reset hidden state\n",
    "        out1 = self.embed(x['s1'])\n",
    "        out2 = self.embed(x['s2'])\n",
    "        batch_size, seq_len = x['s1'].size()\n",
    "        self.hidden1 = self.init_hidden(batch_size).to(device)\n",
    "        self.hidden2 = self.init_hidden(batch_size).to(device)\n",
    "        out1 = torch.nn.utils.rnn.pack_padded_sequence(out1, x['s1_len'], batch_first=True)\n",
    "        out2 = torch.nn.utils.rnn.pack_padded_sequence(out2, x['s2_len'], batch_first=True)\n",
    "        _, self.hidden1 = self.rnn(out1, self.hidden1)\n",
    "        _, self.hidden2 = self.rnn(out2, self.hidden2)\n",
    "        final_hidden1 = self.hidden1.view(1, 2, batch_size, self.hidden_size).transpose(1,2).contiguous().view(batch_size, -1)\n",
    "        final_hidden2 = self.hidden2.view(1, 2, batch_size, self.hidden_size).transpose(1,2).contiguous().view(batch_size, -1)\n",
    "        tmp = torch.zeros(final_hidden2.size()).to(device)\n",
    "        for i in range(batch_size):\n",
    "            tmp[x['s2_order'][i],:] = final_hidden2[i]\n",
    "        final_hidden2 = tmp[list(x['s1_order']),:]\n",
    "        out = torch.max(final_hidden1,final_hidden2)\n",
    "        out = self.linear1(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "middle_size = 200\n",
    "num_epochs =15\n",
    "hidden_size = \n",
    "\n",
    "loss_train_t = [loss_train_t[]]\n",
    "loss_val_t = [loss_train_t[]]\n",
    "acc_val_t = [loss_train_t[]]\n",
    "acc_train_t = [loss_train_t[]]\n",
    "\n",
    "\n",
    "print(\"maxpooling\")\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "\n",
    "model = RNN_max(emb_size, hidden_size, num_layers, middle_size, num_embeddings).to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "for epoch in range(num_epochs):\n",
    "#scheduler.step()\n",
    "    loss = do_train(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    val_loss, val_acc = acc(model, val_loader)\n",
    "    train_loss, train_acc = acc(model,train_loader)\n",
    "    loss_val.append(val_loss)\n",
    "    acc_val.append(val_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(train_acc)\n",
    "    print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "loss_train_t.append(loss_train)\n",
    "loss_val_t.append(loss_val)\n",
    "acc_val_t.append(acc_val)\n",
    "acc_train_t.append(acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_pair_mul(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, middle_size, num_embeddings):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        #\n",
    "        super(RNN_pair_mul, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embed = nn.Embedding(num_embeddings, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        #self.rnn2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(2*hidden_size, middle_size)\n",
    "        self.linear2 = nn.Linear(middle_size, 3)\n",
    "        \n",
    "        self.init_weights()\n",
    "  \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size).float()\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def init_weights(self):\n",
    "        loaded_embeddings[:2, :] = np.random.randn(2,300)\n",
    "        self.embed.weight = nn.Parameter(torch.from_numpy(loaded_embeddings).float())\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reset hidden state\n",
    "        out1 = self.embed(x['s1'])\n",
    "        out2 = self.embed(x['s2'])\n",
    "        batch_size, seq_len = x['s1'].size()\n",
    "        self.hidden1 = self.init_hidden(batch_size).to(device)\n",
    "        self.hidden2 = self.init_hidden(batch_size).to(device)\n",
    "        out1 = torch.nn.utils.rnn.pack_padded_sequence(out1, x['s1_len'], batch_first=True)\n",
    "        out2 = torch.nn.utils.rnn.pack_padded_sequence(out2, x['s2_len'], batch_first=True)\n",
    "        _, self.hidden1 = self.rnn(out1, self.hidden1)\n",
    "        _, self.hidden2 = self.rnn(out2, self.hidden2)\n",
    "        final_hidden1 = self.hidden1.view(1, 2, batch_size, self.hidden_size).transpose(1,2).contiguous().view(batch_size, -1)\n",
    "        final_hidden2 = self.hidden2.view(1, 2, batch_size, self.hidden_size).transpose(1,2).contiguous().view(batch_size, -1)\n",
    "        tmp = torch.zeros(final_hidden2.size()).to(device)\n",
    "        for i in range(batch_size):\n",
    "            tmp[x['s2_order'][i],:] = final_hidden2[i]\n",
    "        final_hidden2 = tmp[list(x['s1_order']),:]\n",
    "        out = torch.max(final_hidden1,final_hidden2)\n",
    "        out = self.linear1(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "middle_size = 200\n",
    "num_epochs =15\n",
    "hidden_size = \n",
    "\n",
    "\n",
    "print(\"pairwise multiplication\")\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "\n",
    "model = RNN_pair_mul(emb_size, hidden_size, num_layers, middle_size, num_embeddings).to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "for epoch in range(num_epochs):\n",
    "#scheduler.step()\n",
    "    loss = do_train(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    val_loss, val_acc = acc(model, val_loader)\n",
    "    train_loss, train_acc = acc(model,train_loader)\n",
    "    loss_val.append(val_loss)\n",
    "    acc_val.append(val_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(train_acc)\n",
    "    print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "loss_train_t.append(loss_train)\n",
    "loss_val_t.append(loss_val)\n",
    "acc_val_t.append(acc_val)\n",
    "acc_train_t.append(acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(loss_train_t[0])+1), loss_train_t[0], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[0])+1), loss_val_t[0], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[0])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"concentenation\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(loss_train_t[1])+1), loss_train_t[1], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[1])+1), loss_val_t[1], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[1])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"max pooling\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(loss_train_t[2])+1), loss_train_t[2], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[2])+1), loss_val_t[2], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[2])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"pairwise multiplication\")\n",
    "\n",
    "plt.savefig(\"interacting_rnn_loss.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(acc_train_t[0])+1), acc_train_t[0], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[0])+1), acc_val_t[0], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[0])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"concentenation\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(acc_train_t[1])+1), acc_train_t[1], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[1])+1), acc_val_t[1], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[1])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"max pooling\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(acc_train_t[2])+1), acc_train_t[2], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[2])+1), acc_val_t[2], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[2])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"pairwise multiplication\")\n",
    "\n",
    "plt.savefig(\"interacting_acc_rnn.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "middle_size = 200\n",
    "num_epochs =15\n",
    "hidden_size = \n",
    "\n",
    "loss_train_t = [loss_train_t[]]\n",
    "loss_val_t = [loss_train_t[]]\n",
    "acc_val_t = [loss_train_t[]]\n",
    "acc_train_t = [loss_train_t[]]\n",
    "\n",
    "\n",
    "print(\"weight decay\")\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "\n",
    "model = RNN(emb_size, hidden_size, num_layers, middle_size, num_embeddings).to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "for epoch in range(num_epochs):\n",
    "#scheduler.step()\n",
    "    loss = do_train(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    val_loss, val_acc = acc(model, val_loader)\n",
    "    train_loss, train_acc = acc(model,train_loader)\n",
    "    loss_val.append(val_loss)\n",
    "    acc_val.append(val_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(train_acc)\n",
    "    print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "loss_train_t.append(loss_train)\n",
    "loss_val_t.append(loss_val)\n",
    "acc_val_t.append(acc_val)\n",
    "acc_train_t.append(acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, num_layers, middle_size, num_embeddings):\n",
    "        # RNN Accepts the following hyperparams:\n",
    "        # emb_size: Embedding Size\n",
    "        # hidden_size: Hidden Size of layer in RNN\n",
    "        # num_layers: number of layers in RNN\n",
    "        # num_classes: number of output classes\n",
    "        #\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embed = nn.Embedding(num_embeddings, emb_size, padding_idx=PAD_IDX)\n",
    "        self.rnn = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        #self.rnn2 = nn.GRU(emb_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(4*hidden_size, middle_size)\n",
    "        self.linear2 = nn.Linear(middle_size, 3)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.init_weights()\n",
    "  \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size).float()\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    def init_weights(self):\n",
    "        loaded_embeddings[:2, :] = np.random.randn(2,300)\n",
    "        self.embed.weight = nn.Parameter(torch.from_numpy(loaded_embeddings).float())\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reset hidden state\n",
    "        out1 = self.embed(x['s1'])\n",
    "        out2 = self.embed(x['s2'])\n",
    "        batch_size, seq_len = x['s1'].size()\n",
    "        self.hidden1 = self.init_hidden(batch_size).to(device)\n",
    "        self.hidden2 = self.init_hidden(batch_size).to(device)\n",
    "        out1 = torch.nn.utils.rnn.pack_padded_sequence(out1, x['s1_len'], batch_first=True)\n",
    "        out2 = torch.nn.utils.rnn.pack_padded_sequence(out2, x['s2_len'], batch_first=True)\n",
    "        _, self.hidden1 = self.rnn(out1, self.hidden1)\n",
    "        _, self.hidden2 = self.rnn(out2, self.hidden2)\n",
    "        final_hidden1 = self.hidden1.view(1, 2, batch_size, self.hidden_size).transpose(1,2).contiguous().view(batch_size, -1)\n",
    "        final_hidden2 = self.hidden2.view(1, 2, batch_size, self.hidden_size).transpose(1,2).contiguous().view(batch_size, -1)\n",
    "        tmp = torch.zeros(final_hidden2.size()).to(device)\n",
    "        for i in range(batch_size):\n",
    "            tmp[x['s2_order'][i],:] = final_hidden2[i]\n",
    "        final_hidden2 = tmp[list(x['s1_order']),:]\n",
    "        out = torch.cat([final_hidden1,final_hidden2], dim=1)\n",
    "        out = self.linear1(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "middle_size = 200\n",
    "num_epochs =15\n",
    "hidden_size = \n",
    "\n",
    "\n",
    "print(\"dropout\")\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "\n",
    "model = RNN(emb_size, hidden_size, num_layers, middle_size, num_embeddings).to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "for epoch in range(num_epochs):\n",
    "#scheduler.step()\n",
    "    loss = do_train(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    val_loss, val_acc = acc(model, val_loader)\n",
    "    train_loss, train_acc = acc(model,train_loader)\n",
    "    loss_val.append(val_loss)\n",
    "    acc_val.append(val_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(train_acc)\n",
    "    print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "loss_train_t.append(loss_train)\n",
    "loss_val_t.append(loss_val)\n",
    "acc_val_t.append(acc_val)\n",
    "acc_train_t.append(acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(loss_train_t[0])+1), loss_train_t[0], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[0])+1), loss_val_t[0], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[0])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"no regularization\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(loss_train_t[1])+1), loss_train_t[1], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[1])+1), loss_val_t[1], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[1])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"weight decay\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(loss_train_t[2])+1), loss_train_t[2], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[2])+1), loss_val_t[2], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[2])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"dropout\")\n",
    "\n",
    "plt.savefig(\"regularization_rnn_loss.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(acc_train_t[0])+1), acc_train_t[0], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[0])+1), acc_val_t[0], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[0])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"no regularization\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(acc_train_t[1])+1), acc_train_t[1], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[1])+1), acc_val_t[1], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[1])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"weight decay\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(acc_train_t[2])+1), acc_train_t[2], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[2])+1), acc_val_t[2], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[2])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"dropout\")\n",
    "\n",
    "plt.savefig(\"regularization_acc_rnn.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORD_LENGTH = 82\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    s1_list = []\n",
    "    s2_list = []\n",
    "    s1_len_list = []\n",
    "    s2_len_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[1])\n",
    "        s1_len_list.append(min(MAX_WORD_LENGTH, datum[0][\"s1_len\"]))\n",
    "        s2_len_list.append(min(MAX_WORD_LENGTH, datum[0][\"s2_len\"]))\n",
    "        s1 = np.pad(np.array(datum[0][\"s1\"][:MAX_WORD_LENGTH]),\n",
    "                                pad_width=((0,max(0, MAX_WORD_LENGTH-datum[0][\"s1_len\"]))),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        s1_list.append(s1)\n",
    "        s2 = np.pad(np.array(datum[0][\"s2\"][:MAX_WORD_LENGTH]),\n",
    "                                pad_width=((0,max(0, MAX_WORD_LENGTH-datum[0][\"s2_len\"]))),\n",
    "                                mode=\"constant\", constant_values=PAD_IDX)\n",
    "        s2_list.append(s2)\n",
    "    ind_dec_order1 = np.argsort(s1_len_list)[::-1]\n",
    "    s1_list = np.array(s1_list)[ind_dec_order1]\n",
    "    s1_len_list = np.array(s1_len_list)[ind_dec_order1]\n",
    "    ind_dec_order2 = np.argsort(s2_len_list)[::-1]\n",
    "    s2_list = np.array(s2_list)[ind_dec_order2]\n",
    "    s2_len_list = np.array(s2_len_list)[ind_dec_order2]       \n",
    "    x = {\n",
    "        \"s1\": torch.from_numpy(np.array(s1_list)).long().to(device),\n",
    "        \"s2\": torch.from_numpy(np.array(s2_list)).long().to(device),\n",
    "        \"s1_len\": torch.LongTensor(s1_len_list).to(device),\n",
    "        \"s2_len\": torch.LongTensor(s2_len_list).to(device),\n",
    "        \"s1_order\": torch.from_numpy(ind_dec_order1.copy()).to(device),\n",
    "        \"s2_order\": torch.from_numpy(ind_dec_order2.copy()).to(device)\n",
    "    }\n",
    "    y = torch.LongTensor(np.array(label_list)[ind_dec_order1])\n",
    "    return x, y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "num_layers = 1\n",
    "middle_size = 200\n",
    "num_epochs = 10\n",
    "hidden_size = 100\n",
    "model = RNN(emb_size, hidden_size, num_layers, middle_size, num_embeddings).to(device)\n",
    "#criterion = torch.nn.NLLLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "#for epoch in range(num_epochs):\n",
    "    #scheduler.step()\n",
    "#    _ = do_train(\n",
    "#            model=model, \n",
    "#            criterion=criterion,\n",
    "#            dataloader=train_loader,\n",
    "#            optimizer=optimizer,\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnli_val = pd.read_csv(\"hw2_data/mnli_val.tsv\", sep = '\\t')\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "def word_emb(x):\n",
    "    return [token2id[i] if i in token2id else UNK_IDX for i in x]\n",
    "genre = mnli_val['genre'].unique()\n",
    "mnli_val_acc = {}\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
    "for gen in genre:\n",
    "    s1_mnli_val = mnli_val.loc[mnli_val['genre']==gen,'sentence1'].values\n",
    "    s2_mnli_val = mnli_val.loc[mnli_val['genre']==gen,'sentence2'].values\n",
    "    label_mnli_val = mnli_val.loc[mnli_val['genre']==gen,'label'].values\n",
    "    label_mnli_val = list(map(one_hot_label, label_mnli_val))\n",
    "    s1_mnli_val = [i.split() for i in s1_mnli_val]\n",
    "    s2_mnli_val  = [i.split() for i in s2_mnli_val]\n",
    "    s1_mnli_val = list(map(word_emb, s1_mnli_val))\n",
    "    s2_mnli_val  = list(map(word_emb, s2_mnli_val))\n",
    "    x_mnli_val = {\"s1\": s1_mnli_val,\n",
    "                  \"s2\": s2_mnli_val}\n",
    "    mnli_val_dataset = MyDataset(x_mnli_val, label_mnli_val)\n",
    "     \n",
    "    BATCH_SIZE = 64\n",
    " \n",
    "    mnli_val_loader = torch.utils.data.DataLoader(dataset=mnli_val_dataset,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                collate_fn=vocab_collate_func,\n",
    "                                shuffle=False)\n",
    "    _, mnli_val_acc_value = acc(model,mnli_val_loader)\n",
    "    mnli_val_acc[gen] = mnli_val_acc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fiction': 0.34371858835220337,\n",
       " 'telephone': 0.36616915464401245,\n",
       " 'slate': 0.3313373327255249,\n",
       " 'government': 0.3868110179901123,\n",
       " 'travel': 0.35132384300231934}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CNN\n",
    "### baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, middle_size,  num_embeddings, kernel_size, padding):\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embed = nn.Embedding(num_embeddings, emb_size, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size*2 , middle_size)\n",
    "        self.linear2 = nn.Linear(middle_size, 3)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        loaded_embeddings[:2, :] = np.random.randn(2,300)\n",
    "        self.embed.weight = nn.Parameter(torch.from_numpy(loaded_embeddings).float())\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out1 = self.embed(x['s1'])\n",
    "        out2 = self.embed(x['s2'])\n",
    "        batch_size, seq_len = x['s1'].size()\n",
    "\n",
    "        out1 = self.conv1(out1.transpose(1, 2)).transpose(1, 2)\n",
    "        out2 = self.conv1(out2.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        out1 = F.relu(out1.contiguous().view(-1, out1.size(-1))).view(batch_size, seq_len, out1.size(-1))\n",
    "        out2 = F.relu(out2.contiguous().view(-1, out2.size(-1))).view(batch_size, seq_len, out2.size(-1))\n",
    "        \n",
    "\n",
    "        out1 = self.conv2(out1.transpose(1, 2)).transpose(1, 2)\n",
    "        out2 = self.conv2(out2.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        out1 = F.relu(out1.contiguous().view(-1, out1.size(-1))).view(batch_size, seq_len, out1.size(-1))\n",
    "        out2 = F.relu(out2.contiguous().view(-1, out2.size(-1))).view(batch_size, seq_len, out2.size(-1))\n",
    "        \n",
    "        out1 = torch.max(out1, dim=1)[0]\n",
    "        out2 = torch.max(out2, dim=1)[0]\n",
    "        \n",
    "        tmp = torch.zeros(out2.size()).to(device)\n",
    "        for i in range(batch_size):\n",
    "            tmp[x['s2_order'][i],:] = out2[i]\n",
    "        out2 = tmp[list(x['s1_order']),:]\n",
    "        \n",
    "        out = torch.cat([out1, out2], dim=1)\n",
    "        \n",
    "        out = self.linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden size = 300\n",
      "Number of parameters is 602203\n",
      "Epoch: [1/15], Train Loss: 0.07387643548607828, Val Loss: 0.08019021928310394, Val Acc: 0.6130000352859497\n",
      "Epoch: [2/15], Train Loss: 0.06615986651301378, Val Loss: 0.07435415089130401, Val Acc: 0.6580000519752502\n",
      "Epoch: [3/15], Train Loss: 0.05936328333020223, Val Loss: 0.07112922763824463, Val Acc: 0.6960000395774841\n",
      "Epoch: [4/15], Train Loss: 0.05185062176048753, Val Loss: 0.0706097549200058, Val Acc: 0.6880000233650208\n",
      "Epoch: [5/15], Train Loss: 0.04721668162107466, Val Loss: 0.07306521856784819, Val Acc: 0.6880000233650208\n",
      "Epoch: [6/15], Train Loss: 0.03967039755642419, Val Loss: 0.07561740696430207, Val Acc: 0.6790000200271606\n",
      "Epoch: [7/15], Train Loss: 0.03209091636121273, Val Loss: 0.0810650485754013, Val Acc: 0.6860000491142273\n",
      "Epoch: [8/15], Train Loss: 0.026664241744279896, Val Loss: 0.08933041548728943, Val Acc: 0.6630000472068787\n",
      "Epoch: [9/15], Train Loss: 0.02188171897262333, Val Loss: 0.09915066361427308, Val Acc: 0.6830000281333923\n",
      "Epoch: [10/15], Train Loss: 0.017909686961621055, Val Loss: 0.10909668266773224, Val Acc: 0.6760000586509705\n",
      "Epoch: [11/15], Train Loss: 0.015034200437664985, Val Loss: 0.11926575386524199, Val Acc: 0.6650000214576721\n",
      "Epoch: [12/15], Train Loss: 0.014184032317847008, Val Loss: 0.13579871249198913, Val Acc: 0.656000018119812\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "middle_size = 200\n",
    "num_epochs =15\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "\n",
    "loss_train_t = []\n",
    "loss_val_t = []\n",
    "acc_val_t = []\n",
    "acc_train_t = []\n",
    "\n",
    "for hidden_size in [100, 300, 500, 700]:\n",
    "    print(\"hidden size = {}\".format(hidden_size))\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    acc_train = []\n",
    "\n",
    "\n",
    "    model = CNN(emb_size, hidden_size, middle_size, num_embeddings, kernel_size, padding).to(device)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "    for epoch in range(num_epochs):\n",
    "    #scheduler.step()\n",
    "        loss = do_train(\n",
    "            model=model, \n",
    "            criterion=criterion,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "        val_loss, val_acc = acc(model, val_loader)\n",
    "        train_loss, train_acc = acc(model,train_loader)\n",
    "        loss_val.append(val_loss)\n",
    "        acc_val.append(val_acc)\n",
    "        loss_train.append(train_loss)\n",
    "        acc_train.append(train_acc)\n",
    "        print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "    loss_train_t.append(loss_train)\n",
    "    loss_val_t.append(loss_val)\n",
    "    acc_val_t.append(acc_val)\n",
    "    acc_train_t.append(acc_train)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(loss_train_t[0])+1), loss_train_t[0], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[0])+1), loss_val_t[0], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[0])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 100\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(loss_train_t[1])+1), loss_train_t[1], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[1])+1), loss_val_t[1], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[1])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 300\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(loss_train_t[2])+1), loss_train_t[2], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[2])+1), loss_val_t[2], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[2])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 500\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(range(1, len(loss_train_t[3])+1), loss_train_t[3], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[3])+1), loss_val_t[3], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[3])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 700\")\n",
    "plt.savefig(\"hidden_cnn_loss.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(acc_train_t[0])+1), acc_train_t[0], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[0])+1), acc_val_t[0], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[0])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 100\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(acc_train_t[1])+1), acc_train_t[1], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[1])+1), acc_val_t[1], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[1])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 300\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(acc_train_t[2])+1), acc_train_t[2], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[2])+1), acc_val_t[2], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[2])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 500\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(range(1, len(acc_train_t[3])+1), acc_train_t[3], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[3])+1), acc_val_t[3], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[3])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"hidden size = 700\")\n",
    "plt.savefig(\"hidden_cnn_acc.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "middle_size = 200\n",
    "num_epochs =15\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "hidden_size = \n",
    "\n",
    "loss_train_t = [loss_train_t[]]\n",
    "loss_val_t = [loss_train_t[]]\n",
    "acc_val_t = [loss_train_t[]]\n",
    "acc_train_t = [loss_train_t[]]\n",
    "\n",
    "for kernel_size in [3, 7, 11, 15]:\n",
    "    print(\"kernel size = {}\".format(kernel_size))\n",
    "    padding = int((kernel_size-1)/2)\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    acc_train = []\n",
    "\n",
    "\n",
    "    model = CNN(emb_size, hidden_size, middle_size, num_embeddings, kernel_size, padding).to(device)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "    for epoch in range(num_epochs):\n",
    "    #scheduler.step()\n",
    "        loss = do_train(\n",
    "            model=model, \n",
    "            criterion=criterion,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "        )\n",
    "        val_loss, val_acc = acc(model, val_loader)\n",
    "        train_loss, train_acc = acc(model,train_loader)\n",
    "        loss_val.append(val_loss)\n",
    "        acc_val.append(val_acc)\n",
    "        loss_train.append(train_loss)\n",
    "        acc_train.append(train_acc)\n",
    "        print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "    loss_train_t.append(loss_train)\n",
    "    loss_val_t.append(loss_val)\n",
    "    acc_val_t.append(acc_val)\n",
    "    acc_train_t.append(acc_train)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(loss_train_t[0])+1), loss_train_t[0], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[0])+1), loss_val_t[0], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[0])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"kernel size = 3\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(loss_train_t[1])+1), loss_train_t[1], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[1])+1), loss_val_t[1], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[1])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"kernel size = 7\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(loss_train_t[2])+1), loss_train_t[2], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[2])+1), loss_val_t[2], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[2])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"kernel size = 11\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(range(1, len(loss_train_t[3])+1), loss_train_t[3], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[3])+1), loss_val_t[3], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[3])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"kernel size = 15\")\n",
    "plt.savefig(\"kernel_cnn_loss.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(acc_train_t[0])+1), acc_train_t[0], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[0])+1), acc_val_t[0], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[0])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"kernel size = 3\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(acc_train_t[1])+1), acc_train_t[1], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[1])+1), acc_val_t[1], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[1])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"kernel size = 7\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(acc_train_t[2])+1), acc_train_t[2], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[2])+1), acc_val_t[2], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[2])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"kernel size = 11\")\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(range(1, len(acc_train_t[3])+1), acc_train_t[3], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[3])+1), acc_val_t[3], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[3])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"kernel size = 15\")\n",
    "plt.savefig(\"kernel_cnn_acc.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ways of interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-d2fd37800a7a>, line 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-d2fd37800a7a>\"\u001b[1;36m, line \u001b[1;32m63\u001b[0m\n\u001b[1;33m    hidden_size =\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class CNN_max(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, middle_size,  num_embeddings, kernel_size, padding):\n",
    "\n",
    "        super(CNN_max, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embed = nn.Embedding(num_embeddings, emb_size, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size , middle_size)\n",
    "        self.linear2 = nn.Linear(middle_size, 3)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        loaded_embeddings[:2, :] = np.random.randn(2,300)\n",
    "        self.embed.weight = nn.Parameter(torch.from_numpy(loaded_embeddings).float())\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out1 = self.embed(x['s1'])\n",
    "        out2 = self.embed(x['s2'])\n",
    "        batch_size, seq_len = x['s1'].size()\n",
    "\n",
    "        out1 = self.conv1(out1.transpose(1, 2)).transpose(1, 2)\n",
    "        out2 = self.conv1(out2.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        out1 = F.relu(out1.contiguous().view(-1, out1.size(-1))).view(batch_size, seq_len, out1.size(-1))\n",
    "        out2 = F.relu(out2.contiguous().view(-1, out2.size(-1))).view(batch_size, seq_len, out2.size(-1))\n",
    "        \n",
    "\n",
    "        out1 = self.conv2(out1.transpose(1, 2)).transpose(1, 2)\n",
    "        out2 = self.conv2(out2.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        out1 = F.relu(out1.contiguous().view(-1, out1.size(-1))).view(batch_size, seq_len, out1.size(-1))\n",
    "        out2 = F.relu(out2.contiguous().view(-1, out2.size(-1))).view(batch_size, seq_len, out2.size(-1))\n",
    "        \n",
    "        out1 = torch.max(out1, dim=1)[0]\n",
    "        out2 = torch.max(out2, dim=1)[0]\n",
    "        \n",
    "        tmp = torch.zeros(out2.size()).to(device)\n",
    "        for i in range(batch_size):\n",
    "            tmp[x['s2_order'][i],:] = out2[i]\n",
    "        out2 = tmp[list(x['s1_order']),:]\n",
    "        \n",
    "        out = torch.max(out1, out2)\n",
    "        \n",
    "        out = self.linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "middle_size = 200\n",
    "num_epochs =15\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "hidden_size = \n",
    "\n",
    "loss_train_t = [loss_train_t[]]\n",
    "loss_val_t = [loss_train_t[]]\n",
    "acc_val_t = [loss_train_t[]]\n",
    "acc_train_t = [loss_train_t[]]\n",
    "\n",
    "\n",
    "print(\"maxpooling\")\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "\n",
    "model = CNN_max(emb_size, hidden_size, middle_size, num_embeddings, kernel_size, padding).to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "for epoch in range(num_epochs):\n",
    "#scheduler.step()\n",
    "    loss = do_train(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    val_loss, val_acc = acc(model, val_loader)\n",
    "    train_loss, train_acc = acc(model,train_loader)\n",
    "    loss_val.append(val_loss)\n",
    "    acc_val.append(val_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(train_acc)\n",
    "    print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "loss_train_t.append(loss_train)\n",
    "loss_val_t.append(loss_val)\n",
    "acc_val_t.append(acc_val)\n",
    "acc_train_t.append(acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_pair_mul(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, middle_size,  num_embeddings, kernel_size, padding):\n",
    "\n",
    "        super(CNN_pair_mul, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.embed = nn.Embedding(num_embeddings, emb_size, padding_idx=PAD_IDX)\n",
    "    \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.linear1 = nn.Linear(hidden_size , middle_size)\n",
    "        self.linear2 = nn.Linear(middle_size, 3)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        loaded_embeddings[:2, :] = np.random.randn(2,300)\n",
    "        self.embed.weight = nn.Parameter(torch.from_numpy(loaded_embeddings).float())\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out1 = self.embed(x['s1'])\n",
    "        out2 = self.embed(x['s2'])\n",
    "        batch_size, seq_len = x['s1'].size()\n",
    "\n",
    "        out1 = self.conv1(out1.transpose(1, 2)).transpose(1, 2)\n",
    "        out2 = self.conv1(out2.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        out1 = F.relu(out1.contiguous().view(-1, out1.size(-1))).view(batch_size, seq_len, out1.size(-1))\n",
    "        out2 = F.relu(out2.contiguous().view(-1, out2.size(-1))).view(batch_size, seq_len, out2.size(-1))\n",
    "        \n",
    "\n",
    "        out1 = self.conv2(out1.transpose(1, 2)).transpose(1, 2)\n",
    "        out2 = self.conv2(out2.transpose(1, 2)).transpose(1, 2)\n",
    "        \n",
    "        out1 = F.relu(out1.contiguous().view(-1, out1.size(-1))).view(batch_size, seq_len, out1.size(-1))\n",
    "        out2 = F.relu(out2.contiguous().view(-1, out2.size(-1))).view(batch_size, seq_len, out2.size(-1))\n",
    "        \n",
    "        out1 = torch.max(out1, dim=1)[0]\n",
    "        out2 = torch.max(out2, dim=1)[0]\n",
    "        \n",
    "        tmp = torch.zeros(out2.size()).to(device)\n",
    "        for i in range(batch_size):\n",
    "            tmp[x['s2_order'][i],:] = out2[i]\n",
    "        out2 = tmp[list(x['s1_order']),:]\n",
    "        \n",
    "        out = out1 * out2\n",
    "        \n",
    "        out = self.linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "learning_rate = 0.001\n",
    "emb_size = 300\n",
    "middle_size = 200\n",
    "num_epochs =15\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "hidden_size = \n",
    "\n",
    "\n",
    "print(\"pairwise multiplication\")\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "acc_val = []\n",
    "acc_train = []\n",
    "\n",
    "\n",
    "model = CNN_pair_mul(emb_size, hidden_size, middle_size, num_embeddings, kernel_size, padding).to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(\"Number of parameters is {}\".format(get_n_params(model)))\n",
    "for epoch in range(num_epochs):\n",
    "#scheduler.step()\n",
    "    loss = do_train(\n",
    "        model=model, \n",
    "        criterion=criterion,\n",
    "        dataloader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    val_loss, val_acc = acc(model, val_loader)\n",
    "    train_loss, train_acc = acc(model,train_loader)\n",
    "    loss_val.append(val_loss)\n",
    "    acc_val.append(val_acc)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(train_acc)\n",
    "    print('Epoch: [{}/{}], Train Loss: {}, Val Loss: {}, Val Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, train_loss, val_loss, val_acc))\n",
    "        \n",
    "loss_train_t.append(loss_train)\n",
    "loss_val_t.append(loss_val)\n",
    "acc_val_t.append(acc_val)\n",
    "acc_train_t.append(acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(loss_train_t[0])+1), loss_train_t[0], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[0])+1), loss_val_t[0], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[0])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"concentenation\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(loss_train_t[1])+1), loss_train_t[1], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[1])+1), loss_val_t[1], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[1])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"max pooling\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(loss_train_t[2])+1), loss_train_t[2], label=\"Training Loss\")\n",
    "plt.plot(range(1, len(loss_val_t[2])+1), loss_val_t[2], label=\"Validation Loss\")\n",
    "plt.xticks(range(1, len(loss_val_t[2])+1))\n",
    "plt.ylim(0, 0.2)\n",
    "plt.legend()\n",
    "plt.title(\"pairwise multiplication\")\n",
    "\n",
    "plt.savefig(\"interacting_cnn_loss.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(221)\n",
    "plt.plot(range(1, len(acc_train_t[0])+1), acc_train_t[0], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[0])+1), acc_val_t[0], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[0])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"concentenation\")\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(range(1, len(acc_train_t[1])+1), acc_train_t[1], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[1])+1), acc_val_t[1], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[1])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"max pooling\")\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(range(1, len(acc_train_t[2])+1), acc_train_t[2], label=\"Training Acc\")\n",
    "plt.plot(range(1, len(acc_val_t[2])+1), acc_val_t[2], label=\"Validation Acc\")\n",
    "plt.xticks(range(1, len(acc_val_t[2])+1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.title(\"pairwise multiplication\")\n",
    "\n",
    "plt.savefig(\"interacting_cnn_acc.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
